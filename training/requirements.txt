# Training Dependencies for TinyLlama Fine-tuning
# ==============================================

# Core ML/AI frameworks
torch>=2.1.0
transformers>=4.35.0
datasets>=2.14.5
accelerate>=0.24.1
tokenizers>=0.15.0

# Fine-tuning and optimization
peft>=0.6.2
bitsandbytes>=0.41.3
trl>=0.7.4
optimum[onnxruntime]>=1.15.0

# Experiment tracking and monitoring
wandb>=0.16.0
tensorboard>=2.15.0

# Data processing and utilities
pandas>=2.1.0
numpy>=1.24.0
scikit-learn>=1.3.0
faker>=20.1.0
beautifulsoup4>=4.12.0
requests>=2.31.0

# Development and testing
pytest>=7.4.0
jupyter>=1.0.0
ipywidgets>=8.1.0
matplotlib>=3.7.0
seaborn>=0.12.0

# Hardware acceleration (optional)
# Uncomment based on your setup:
# torch-audio  # For CPU-only training
# torch-cuda   # For CUDA GPU training
# torch-mps    # For Apple Silicon GPU training

# Model conversion and deployment
onnx>=1.15.0
onnxruntime>=1.16.0

# Progress bars and CLI utilities
tqdm>=4.66.0
click>=8.1.0
rich>=13.7.0